services:
  # PostgreSQL database for Airflow metadata and application data
  db:
    container_name: byte_sanctuary_db
    image: postgres:14.19
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - byte_sanctuary_volume:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d  # For custom initialization
    ports:
      - "5434:5432"  # Different port to avoid conflict with existing postgres
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${DB_USER}", "-d", "${DB_NAME}"]
      interval: 5s
      retries: 5
    restart: always

  # Redis for Airflow Celery backend
  redis:
    container_name: redis
    image: redis:7.2.0
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  # Backend FastAPI service
  backend:
    build: ./backend
    container_name: backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://airflow:airflow@db:5432/airflow
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./backend:/app
    restart: always

  # Airflow initialization service
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Waiting for PostgreSQL and Redis..."
        sleep 10
        airflow db migrate
        airflow users add-user \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || echo "Admin user already exists"
        echo "Airflow initialization complete!"
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    user: "50000:0"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Airflow webserver
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    user: "50000:0"
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # Airflow scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    user: "50000:0"
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # Airflow worker
  airflow-worker:
    build: ./airflow
    container_name: airflow-worker
    command: worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      - DUMB_INIT_SETSID=0
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    user: "50000:0"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # Airflow API server (alternative to webserver)
  airflow-api:
    build: ./airflow
    container_name: airflow-api
    command: api-server
    ports:
      - "8081:8080"  # Different port to avoid conflict with webserver
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    user: "50000:0"
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # Frontend server
  frontend-server:
    build: ./frontend/server
    container_name: frontend-server
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - backend
    volumes:
      - ./frontend/server:/app
    restart: always

  # Frontend client
  frontend-client:
    build: ./frontend/client
    container_name: frontend-client
    ports:
      - "3001:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    depends_on:
      - backend
    volumes:
      - ./frontend/client:/app
    restart: always

  # DBT service
  dbt:
    build: ./dbt_stock_analyzer
    container_name: dbt
    volumes:
      - ./dbt_stock_analyzer:/usr/app/dbt
      - ~/.dbt:/root/.dbt
    environment:
      - DBT_PROFILES_DIR=/root/.dbt
    depends_on:
      - db
    restart: on-failure

volumes:
  byte_sanctuary_volume: